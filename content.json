[{"title":"Quora Insincere Questions Classification","date":"2019-01-12T16:00:00.000Z","path":"2019/01/13/Quora比赛/","text":"Kaggle 的 NLP 比赛 Quora Insincere Questions Classification 总结 比赛简介这是一个二分类的问题, 目标是给定的Quora中的问题文本序列, 判断该问题是否为一个真诚的问题(insincere classification)训练集样本数1306122条Quora问题文本序列, \b第一阶段测试集样本数56370条. 这是一个kernel-only的比赛, 即所有的\b提交结果必须通过kaggle提供的kernel环境执行\b生成(如果使用Telsa K80显卡则限kernel运行时间2h, 不使用显卡则为6h, 内存16G), 此外不允许连接互联网及使用任何外部数据, Kaggle官方提供了4个Embedding文件, 包括GloVe、Paragram、WikiNews(Fasttext)和GoogleNews(Word2Vec)四种预训练词向量 关于四种词向量的简介: Word2vec: word2vec严格来说是一个工具, 大部分情况下我们提到word2vec都是指其Skip-Gram结构所生成的词向量模型来指代word2vec, Skip-Gram结构利用中间词生成上下文单词的向量表示, 而另一个结构CBOW则是根据上下文单词向量表示学习中间词的向量表示. word2vec的两个训练trick: 1)层次softmax, 层次softmax替代传统softmax加速计算, 相比传统softmax, 层次softmax实际上就是根据预料中单词的词频来构建一个霍夫曼树, 通过根节点到叶节点的路径来求出概率, 把N分类问题转换成logN次二分类问题; 2)负采样, 负采样是在softmax计算时不使用所有的单词, 而是采样指定数量的负样本单词. Fasttext: fasttext和word2vec都是同一个作者提出的, 它和word2vec的CBOW模型结构类似但是fasttext是个分类模型, 词向量是其训练过程中的副产物. fasttext的输入不止是上下文的词向量还包含上下文单词的ngram特征, 经过叠加平均后经过一个线性映射得到一个文档向量(隐层), 随后输出输入词序列所属不同类别的概率. fasttext最大的特点就是快, 也采用了分层softmax和负采样技术来提升训练速度, 且除了输出层外均使用线性激活函数, 训练的更快. fasttext比word2vec快的地方在于: word2vec对一条句子中的每个单词都要做一次训练, 而fasttext直接对一条语句训练, 此外, 输出层的霍夫曼树的叶节点个数也小于word2vec(前者的叶节点是类别, 后者是不同的单词) GloVe: GloVe由Stanford大学的NLP Group在2014年发表的论文中提出的, 与word2vec和fasttext不同, 其研究对象是单词之间的共现率, 先构造一个共现矩阵X, X_{ij}表示上下文单词i和j在特定上下文窗口大小下的共同出现的次数, 距离越远的单词其共现率的计算权重越小. 其损失函数核心部分是一个平方损失函数(w_i^T * \\tilde{w_j} + b_i + \\tilde{b_j} - log(X_{ij}))^2, w_i和w_j即为目标词向量, 这个损失函数的含义是希望两个共现的单词的表示应该和他们共现的频率的对数尽可能的接近. Paragram: paragram是在paraphrase语料库上利用Skip-Gram模型得到的词向量为基础输入RNN进行fine-tuning训练得到词向量, 其本身并没有什么特殊之处. 整体流程预处理预处理中包含了\b数据清洗、文本序列/向量化等操作 数据清洗由于Quora官方的给数据相对来说比较”干净”, 数据清洗就主要是对特殊字符和拼写错误单词的处理以便经过预处理后, 单词能够在所给的Embedding文件中找到对应的向量表示我们所做的清洗包括: 把特殊字符诸如”, . ! ? + -“等单独隔开防止影响到单词的表示 把”what’s, I’ve”等分开为”what is, I have” 把”qoura, qouta”等错误拼写的单词替换为正确的拼写单词 把所有非字母的字符全部移除并填充缺失值 序列/向量化向量化的目的是为了把输入的文本序列转换成一个序列化、向量化的表示以便于时序模型如LSTM和GRU读取训练. 我们使用keras的Tokenizer来进行文本序列化处理, 简单来说Tokenizer需要我们给定一个参数num_words表示需要从预料中保存出现频率最高的num_words个单词用于生成向量. 实际上Tokenizer就是在所提供的预料数据上使用fit_on_texts 方法生成一个字典, 使得每个单词有着对应的word_index, 再使用texts_to_sequences方法将每条预料转换成对应的向量表示, 这个向量表示实际上就是这条预料中的单词出现在字典中的index, 在后续的深度模型中我们会使用字典生成预训练词向量矩阵, 随后根据这个index向量来获取各个单词对应的词向量(如果单词不存在于预训练词向量中, 则用所有词向量的均值和标准差作为高斯分布函数 np.random.normal 的参数生成相应的向量表示). 在比赛中我们尝试不同的num_words最终设置了num_words = 950001234567somestr = ['ha ha gua angry','howa ha gua excited naive']tok = tt.Tokenizer()tok.fit_on_texts(somestr)tok.word_indexOut[90]: &#123;'angry': 3, 'excited': 5, 'gua': 2, 'ha': 1, 'howa': 4, 'naive': 6&#125;tok.texts_to_sequences(somestr)Out[91]: [[1, 1, 2, 3], [4, 1, 2, 5, 6]] 填充序列保持长度一致经过Tokenizer对每条样本的向量化处理后还有一个问题就是各条处理后的训练样本长度是不一致的, 这样无法直接被keras的RNN模型所使用, 所以我们采用了keras的pad_sequence方法对所有的训练数据做填充/截断处理, 我们统计了所有训练数据的平均长度约为70, 所以我们将所有的样本按照长度70来填充/截断. 模型选择模型选择花费了我们很多时间尝试, 由于限制了kernel的使用时间, 所以模型方面不能太复杂, 我们主要在特征工程 + 传统机器学习模型和深度模型之间做实验选择 传统机器学习模型由于这个比赛和提取语义有很大关系, 传统的机器学习模型不是很适用, 我们尝试过特征工程加上LightGBM, 但是LB得分很不如人意所以我们基本放弃了传统机器学习的思路. 深度模型深度模型在实际使用中表现最好, 很遗憾由于不能使用外部数据导致一些state-of-art的模型比如NAACL 2018的outstanding paper ELMO、ACL 2018上发表的ULMFiT以及Google AI Language与2018年10月提出的BERT都无法使用, 我们在线下测试了这些模型的表现十分出色, 但是线上无法使用也无法得知最终的表现会如何, 在线上我们尝试过很多模型包括: 单模型: CNN, LSTM, GRU, SRU, QRNN以及IndRNN 多模型结合: CNN+LSTM, LSTM+GRU, GRU+LSTM, LSTM+SRU, BiLSTM+BiGRU+Attention+Pooling, BiLSTM+BiGRU+Capsule+CRF 多任务模型: 不仅仅以区分出insincere问题为目标, 我们可以在训练的过程中加入其他的目标同时学习, 例如先对每条数据进行简单的语义分析得出一个情感分数, 把这个分数作为标签与原来的insincere标签一起作为新的标签, 通过联合的训练两个目标带来的好处包括: 1. 加入了更多的特征信息, 降低过拟合的风险; 2. 模型在学习的过程中可能会获取一些额外的信息以供原目标任务参考.最终确定确定的模型框架如下图: 采用五折交叉验证 关于Embedding预训练文件的使用: Embedding文件在前面提到有四种, 我们尝试了不同Embedding文件单独使用的效果发现GloVe和Paragram的单文件效果最好, fasttext和word2vec的效果相对较差, 所以我们选定GloVe和Paragram作为最终使用的Embedding文件, 四种Embedding文件表现差异的原因我们推测是: GloVe和Paragram较word2vec和fasttext在学习词向量时除了局部信息(word2vec和fasttext主要考虑n-gram窗口内的单词信息)外还关注全局信息.此外关于如何使用这两个文件, 我们尝试过取平均和拼接两种方式, 发现针对不同的网络结构两种操作各有胜负, 但整体来说取平均的得分更高且更节省内存空间. 为什么是LSTM+GRU? 因为我们测试过不同的组合, 发现LSTM层后紧接着一个GRU层的表现最佳. GRU+LSTM效果也明显差于前者. 关于LSTM和GRU的原理网上有很多介绍, 这里不赘述. 关于Attention: Attention机制最先在计算机视觉中被应用于图片识别的问题, 之后在自然语言处理(NLP)和计算机视觉(CV)中经常结合递归神经网络结构RNN、GRU、LSTM等深度学习算法, 被称之为Recurrent Attention Model(RAM), 其核心就是一个Encoder-Decoder的过程, 本质就是加权. Google Translate团队在论文 中将Attention定义为: 一个查询和键值对映射到输出的方法，Q、K、V均为向量，输出通过对V进行加权求和得到，权重就是Q、K相似度. 传统的encoder不论输入长短都将输入映射成一个固定维度的隐向量, 这样在序列很长时模型难以学习到合理的隐向量表示, 且输入序列的每个时刻 x 对输出 y 影响都相同, 这显然不合理. 引入Attention之后, 每个输入时刻 x_i 都对应一个中间隐状态 h_i, 其相对于 t 时刻输出的Attention score计算方法为: t 时刻输出的隐状态 H_t 和 每个输入时刻隐状态 h_i 进行点积运算得到. 常用Attention机制还包括Self-Attention以及Multi-heads Attention, 前者不是考率输入和输出之间的注意力权重, 而是仅考虑输入/输出序列本身之间的注意力关系, Self-Attention在计算过程中会直接将输入(出)中任意两个时刻的关系直接通过一个计算步骤直接联系起来而无需考虑距离，因此解决了长距离依赖问题. 除此外，Self-Attention对于增加计算的并行性也有直接帮助作用. 而Multi-heads Attention则可以简单的理解为多次Attention的结果进行拼接, 每次Attention的参数矩阵 W 不一样而已. 关于Capsule: hello 关于CRF: 定义 y: 隐状态 x: 观测值 1) HMM: 隐马尔可夫模型(Hidden Markov model, HMM)是一个生成式模型, 其对隐状态和观测值联合建模, HMM 假设观测序列仅在相邻位置存在依赖关系, 引入了 p(y_i | y_{i-1}) 表隐状态转移概率, 那么 HMM 的公式表示为: p(\\overrightarrow{y}, \\overrightarrow{x}) = \\prod_{i=1}^n p(y_i | y_{i-1})p(x_i | y_i)p(\\overrightarrow{x}) = \\sum_{y \\in \\mathcal{Y}}\\prod_{i=1}^n p(y_i | y_{i-1})p(x_i | y_i)其中, p(y_i | y_{i-1}) 表示由隐状态 y_{i-1} 转移到 y_i 的转移概率, p(x_i | y_i) 表示由隐状态 y_i 所能得到的可能的观测值 x_i 的概率. 累乘则是对观测序列的一种表示. 实际使用中, HMM 的参数 \\lambda = (\\mathcal{Y}, X, \\pi, A, B), \\mathcal{Y} 是隐状态集合, X 是观测值集合, \\pi 是初始状态概率, A 是转移概率 p(y_i | y_{i-1}) 矩阵, B 是观测值概率 p(x_i | y_i) 矩阵, 常见的 POS 任务中, X 就是观察到的句子, Y 是预测标注序列. HMM的缺陷是其基于观察序列中的每个元素都相互条件独立的假设. 即在任何时刻观察值仅仅与隐状态(即要标注的标签)有关. 对于简单的数据集, 这个假设倒是合理. 但大多数现实世界中的真实观察序列是由多个相互作用的特征和观察序列中较长范围内的元素之间的依赖而形成的. 而条件随机场(conditional random fiel, CRF)恰恰就弥补了这个缺陷.2) MEMM:最大熵马尔科夫模型(Maximum Entropy Markov Model, MEMM)是一个判别式模型, 在 HMM 中, 观测值 x_i 仅依赖于隐状态 y_i, 但是实际情况中观测序列还需要很多其他特征来刻画, 为此提出 MEMM 模型允许”定义特征”来直接学习条件概率: p(y_i | y_{i-1}, x_i), 对于整个序列则为: p(Y | X) = \\prod_{t=1}^n p(y_i | y_{i-1}, x_i)其中, p(y_i | y_{i-1}, x_i) 通过最大熵模型来建模: p(y_i | y_{i-1}, x_i) = \\frac{1}{Z(x_i, y_{i-1})} e^{\\sum_a \\lambda_a f_a(x_i, y_i)}其中, \\frac{1}{Z(x_i, y_{i-1})} 是归一化部分, f_a(x_i, y_i) 是特征函数, 函数需要人为定义, \\lambda 是特征函数的权重, 通过训练得到.MEMM 与 HMM 不同之处在于隐状态 y_i 不仅与观测值 x_i 有关, 还与上一时刻的隐状态 y_{i-1} 有关, MEMM 的流程:step 1. 预定义特征函数 f_a(x_i, y_i)step 2. 在数据集上训练模型step 3. 用训练好的模型做序列标注或序列概率问题MEMM 的最大问题在于标记偏置陷入局部最优, 其原因在于归一化部分是局部归一化, 这个问题同样被 CRF 所解决.3) CRF:条件随机场(Conditional Random Fields, CRF)是一个判别式模型, 其定义为给定随机变量 X (如观测序列) 条件下, 随机变量 Y (对应的隐状态序列) 的马尔科夫随机场. 一般在 NLP 领域中我们说的 CRF 都是指链式 CRF 如下图所示: 其公式表示为: p(Y | X) = \\frac{1}{Z(X)}\\prod_i \\psi_i(Y_i | X) = \\frac{1}{Z(X)}\\prod_i e^{\\sum_k \\lambda_k f_k(X, Y_{i-1}, Y_i, i)} = \\frac{1}{Z(X)}e^{\\sum_i \\sum_k \\lambda_k f_k(X, Y_{i-1}, Y_i, i)}这个公式的含义就是给定整个观测序列X = {x_1, x_2, ..., x_i}的条件下, 计算出隐状态序列Y = {y_1, y_2, ..., y_i}的概率, \\frac{1}{Z(X)}是全局归一化解决 MEMM 模型的标记偏置问题, 下标\u001ck表示构造的第几个特征函数, 每个特征函数的权重是\\lambda_k. CRF 的使用流程和 MEMM 类似. 最后总结一下 HMM、 MEMM 和 CRF:HMM 假设观测值 x_i 之间严格独立, 隐状态 y_i 仅与前一时刻的隐状态 y_{i-1} 有关, 观测值 x_i 仅依赖于隐状态 y_i; MEMM 根据观测序列 X 最大化隐状态(输出)序列 Y 的条件概率 p(Y | X), 其假设隐状态 y_i 不仅与前一时刻的隐状态 y_{i-1} 有关, 还和当前观测值 x_i 有关; CRF 则是根据观测序列 X 最大化当前时刻的条件概率 p(Y_i | X), 其他部分和 MEMM 相似.CRF不仅解决了HMM输出独立性假设的问题, 还解决了MEMM的标注偏置问题, MEMM容易陷入局部最优是因为只在局部做归一化, 而CRF统计了全局概率, 在做归一化时考虑了数据在全局的分布吗, 而不是仅仅在局部归一化, 这样就解决了MEMM中的标记偏置的问题, 使得序列标注的解码变得最优解. 但是 CRF 的缺点就是参数多计算量大复杂度高. 关于SRU、QRNN和IndRNN: SRU设计动机是解决 RNN的训练速度太慢、模型的应用性和实验的可重复性、网络结构的可解释性三个问题, 按照论文里的实验显示, SRU最高可以比keras优化的CuDNNLSTM快10倍! 但是实际使用过程中我们发现在该任务上SRU的keras实现速度是比不上CuDNNLSTM的, 不知道是keras实现的代码不够完善还是我们的网络结构不利于SRU的发挥所致, SRU的详细介绍在这 QRNN设计动机是RNN在计算时，有时间的依赖性，并行度受限；而CNN受制于有限的receptive field, 因此, 信息传递太慢, QRNN希望能综合RNN和CNN的优点，尽量避免各自的缺陷, 但是在我们使用的过程中发现精度较低且速度没有明显加快, QRNN的详细介绍在这 IndRNN既可以利用CNN的多层网络，又可以利用RNN的循环连接. 特别适合于解决时序相关问题，比如视频分析，动作识别，NLP之类. 同样和SRU以及QRNN一样精度低于CuDNNLSTM/GRU, 需要堆叠多层才能达到相同精度但速度因此降低导致无法使用, 关于IndRNN的详细介绍在这 关于ELMO、GPT和BERT: ELMO、GPT和BERT都是无法使用的额外数据, 由于BERT的主要对比对象就是 ELMO 和 GPT, 所以在这里简要介绍一下ELMO 和 GPT模型的原理, 然后介绍 BERT. ELMO: 以往获取词向量做法是通过语言模型训练词向量, 其缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. ELMO是双向语言模型BiLM, 联合最大化前向和后向预测单词概率, 具体做法是将句子输入多层BiLSTM获取BiLSTM的不同层内部隐状态进行softmax得到预测单词的表示(多层隐状态加权), 其效果不错但是速度比较慢(ELMO的词向量不像之前与训练模型是fixed, 需要实时的输入句子获取表示, 实际应用中大多是把ELMO的输出和传统词向量的训练输出concat). ELMO 的两大缺陷在于: 一是没有使用大家普遍认为提取特征能力更强的 Transformer 替代 LSTM 来提取特征; 二是 ELMO 的双向拼接特征融合不如 BERT 的一体化特征融合方式. GPT: GPT 是 Open AI 提出的, 简而言之, GPT 和 ELMO 类似是个两阶段模型, 先预训练语言模型再根据这个预训练模型 fine-tuning 完成下游任务. 区别于 ELMO, GPT 使用 Transformer 来作为特征提取器, 但是 GPT 的语言模型是单向的(只使用预测单词之前的单词). 其缺点在于语言模型是单向的, 这无疑会损失很多信息. 此外, GPT 的下游任务的网络结构设计也较麻烦需要和 GPT 模型的网络结构靠拢. ELMO 和 GPT 的出现极大的启发了 BERT, 下面介绍 BERT 模型. BERT: 首先, BERT的出现证明了一个非常深的模型可以显著提高NLP任务的准确率, 而这个模型可以从无标记数据集中预训练得到, 可以算是NLP领域中一个”里程碑”式的发现, 使得NLP领域也有着像 CV 方向的 ResNet 一样的深层骨干预训练网络(之前用的大多都是浅层语言模型的预训练数据如 ELMO、GloVe、Fasttext等). 要了解BERT, 首先要知道Transformer, 在这里简要介绍下 Transformer, Transformer 也是在大名鼎鼎的 中提出的, Transformer 也属于 encoder-decoder 框架, 输入序列首先经过 word embedding, 再和 positional encoding(Transformer 是不考虑序列顺序的, 通过 positional encoding 将词在序列中的位置信息编码成向量)相加后, 输入到 encoder 中. 输出序列经过的处理和输入序列一样, 然后输入到 decoder. 最后, decoder 的输出经过一个线性层, 再接 Softmax. encoder和decoder都有6层, 如下图所示, Transformer 的核心就是 self-attention 和 multi-head attention, 前者用于替代RNN/CNN结构来捕捉长距离依赖, 后者则是使用不同的 attention 矩阵参数进行多次 self-attention 操作拼接后作为最终的 self-attention 输出. 由于需要堆叠多层 self-attention 才能获取满意的结果, 在这个比赛中我们受时间的限制无法使用 Transformer. 左半部分的Nx代表encoder中一层的结构, 每层包含两个部分, 第一部分是 multi-head self-attention, 另一部分是一个前馈神经网络, 两个部分都有一个残差连接和 layer normalization; 右半部分的Nx代表 decoder 中一层的结构, 每层包含三个部分, 前馈神经网络部分和 encoder 一样, multi-head context-attention 部分如其名是指 encoder 和 decoder 之间的 attention 该部分的输入来自 encoder, 除此之外还有一个 masked multi-head self-attention, 所谓 masked 就是对一些位置进行掩盖如置0以获取不同的效果. 回到BERT本身, 有了前人的工作 ELMO 和 GPT, 使得 BERT 可以做到取其精华去其糟粕. BERT 和 ELMO 以及 GPT 一样都是两阶段模型, 先预训练语言模型, 然后在此基础上进行 fine-tuning 完成下游任务. BERT 和 ELMO 以及 GPT 的关系如下图: 对比可以看出, BERT 和 GPT 十分相似, 只不过 BERT 是双向语言模型. BERT 的模型创新性并不强, 其关键就在于如何构造双向 Transformer 语言模型. 这里就涉及到两个技术: 1) Masked LM: 所谓 Masked LM 其本质就是 CBOW 的改良版, CBOW 是用上下文预测中间词, 而 Masked LM 则随机的把15%的单词抠掉(用 mask 掩码替换), 然后去预测被抠掉的单词, 但是有个问题是如果这样做会让模型认为输出是针对这些 mask 掩码的, 所以这15%的单词中只有80%被真正替换成掩码, 剩下的10%替换成随机单词, 还有10%则保持不动, 这样既可以同时利用左右上下文的同时还能避免模型学习目标紊乱. 2) Next Setence Prediction: 所谓 NSP 技术就是做预训练时, 分两种情况选择两个句子, 一是选择语料中真正相连的两个句子, 另一种是第二个句子随机从语料中摘取拼接到第一个句子后面, 除了传统语言模型的预测任务之外, 这里会附带做一个句子关系预测, 判断第二个句子是不是和第一个相关. 这样做既有利于下游句子关系判断的任务,又把预训练部分变成一个多任务过程, 算是一个创新点. 关于ULMFiT: BERT: 首先, BERT的出现证明了一个非常深的模型可以显著提高NLP任务的准确率, 而这个模型可以从无标记数据集中预训练得到, 可以算是NLP领域中一个”里程碑”式的发现, 使得NLP领域也有着像CV方向的ResNet一样的骨干预训练网络(之前用的大多都是浅层语言模型的预训练数据如ELMO、GloVe、Fasttext等). 参数优化参数优化部分主要是学习率的衰减改变、目标函数的改变等, 如CLR、快照集成Snapshot、余弦退火集合热重启SGDR和随机加权平均SWA以及目标函数的改进, 何恺明针对不平衡类别设计的Focal Loss, 还有Google Brain团队在ICLR 2018上发表的论文, 不对学习率进行衰减调整而对batch_szie做文章的技巧.Trick 1. CLR(Cyclical Learning Rate, 周期学习率): 是Leslie Smith在2015年的一篇论文中提到的动态调整学习率方案, 以往调整学习率的方案大多是逐步降低或使用指数函数从而使得模型趋于稳定, 但是Smith认为与其单调降低学习率不如让学习率在合理的范围内进行周期性变化, 这样可以以更少的训练步骤提高模型精度. Smith认为最佳学习率出在base_lr和max_lr之间, 其背后的原理可以简单的理解为: 如果最佳学习率在[base_lr, max_lr]之间周期变化, 那么大多数情况下会得到一个接近最有学习率的学习率. 此外, 还有利于模型摆脱鞍点. CLR的主要参数为: base_lr(学习率波动区间的最小值)、max_lr(学习率波动区间的最大值, 不一定会达到)、step_size(半个学习率循环的步长, 作者建议取值为训练轮数 = 训练样本数 / batch_size 的2~8倍)、mode(学习率变化的模式, 包括三角变化和指数变化)以及一个指数模式下的衰减参数gamma. Trick 2. Snapshot(快照集成): 发表在ICLR 2017上的论文, 是一种网络集成方法只需要训练一个模型, 使用基于余弦退火策略的循环学习率策略. 其motivation在于: 训练过程中的不同局部最优解可能包含更多的信息.我们知道SGD会在权重空间内产生较大的跃变, 当学习率因为余弦退火策略而下降时, SGD会收敛到某个局部解, 此时Snapshot集成就会对这个局部解模型拍下”快照”, 将局部解加入解集合中. 随后学习率再次被重置为一个较高值(warm restart), SGD再次收敛时很可能产生一个大的跃变进入新的局部解. 当然为了找到具有差异性的模型, 需要设置较大的epochs.(20~40) Trick 3. SGDR(Stochastic gradient descent with warm restarts, 余弦退火结合热重启): 同样发表在ICLR 2017上, 按照我的理解SGDR和Snapshot的区别在于Snapshot利用和SGDR一样的过程调整学习率即余弦退火结合循环学习率策略, 只不过Snapshot在每次收敛到局部解时保存了局部解的模型用与ensemble. Trick 4. SWA(Stochastic Weight Averaging, 随机加权平均): 是2018年提出的一篇论文, 它和主要对比对象快速几何集成FGE(Fast Geometric Ensembling, 快速集合集成)都投了NIPS 2018, 但是FGE的论文中了, 这篇却没有(实验结果比不过不可怕, 开会缺谁谁尴尬 =_=#).我们没有使用FGE, 这里就简单介绍一下FGE: FGE和Snapshot集成十分相似但主要有两个不同点: 1) FGE使用线性分段周期学习率策略而不是余弦退火; 2) FGE的循环周期短得多(相较于Snapshot的20~40降低到2~4个epochs). 直观上来看这样获取的模型差异性可能比较小, 但是作者发现在足够多的不同模型之间, 存在低loss的连接通路(如下图所示, 实线部分即为局部最优解之间的低loss通路), 沿着这些通路以较小的步长前进保存模型快照可以获取差异足够大的模型, 相比Snapshot的集成结果FGE的表现更好且速度更快. 回到SWA, SWA只需要FGE的一小部分算力, 严格来说SWA不算集成模型, 在训练结束后SWA只有一个模型, 性能优于Snapshot接近FGE. SWA的实验结果如下图, 图中左部是三个FGE的快照模型结果和SWA在权重空间内平均的结果, 图中部是SWA与SGD的泛化误差差别, 注意SWA的训练误差是高于SGD的(图右部). SWA设计的思路源于观察, 每次学习率周期学习到的结果倾向于堆叠在低loss平面边缘(图左的3个FGE结果: W_1, W_2, W_3), 对这样的结果取平均可能得到一个更好泛化解(图左部的 W_{SWA}). SWA只保存两个模型, 一个模型保存对训练过程中不同的模型权值平均值(W_{SWA}), 它将用于最终的预测; 另一个模型(W)用于探索权值空间. 其更新公式: W_{SWA} \\leftarrow \\frac{W_{SWA} \\cdot n_{models} + W}{n_{models} + 1}Trick 5. Focal Loss: Focal loss是何恺明团队提出的一个处理类别不平衡的损失函数, 可以改善图像物体检测的效果. 虽然是针对CV提出的一个不平衡损失函数, 但是在NLP中也存在大量类别不平衡任务, 比如这个比赛中正负样本的比例就达到1: 15. 下面简单介绍Focal loss设计的motivation: 1) 极度的样本不平衡, 正负比例可能大于1000; 2) 梯度被easy example支配(虽然easy example的loss很低, 但是数量过多导致最终loss依然很大从而收敛不到一个好的结果). 处理方法也很简单就是对easy example按loss值衰减其权重, 如下图所示well-classified部分就是easy examples, CE为传统交叉熵损失, FL为Focal loss损失, 其损失函数公式为: FL(p_t) = -\\alpha_t(1 - p_t)^\\gamma log(p_t)其中 \\alpha_t 是均衡系数, \\gamma 是对easy example的衰减超参数, 当 p_t 趋于1时对应的loss很低此时通过 (1 - p_t)^\\gamma 项来衰减其在损失函数中的权重. Trick 6. Don’t decay the learning rate, increase your batch size!: 这是ICLR 2018的论文, 作者也没起个好记的名字, 我就只好把论文名字放这了, 看起来还是很有气势的. 不同于衰减学习率, 作者提出了在增加Batch Size的同时保持学习率的策略, 既可以保证test中不掉点, 还可以减少参数更新的次数; 作者表示还可以既增加学习率又增大Batch Size, 如此可以基本保持test中不掉点又进一步减少参数更新次数. 作者在论文中给出了详细的实验对比, 有兴趣的可以看看, 我们在比赛中尝试了这样的做法, 没什么效果. 踩过的坑深度模型的可重复性由于使用到了CuDNNLSTM和CuDNNGRU, 其网络权重初始化具有很大的随机性, 使得同样的代码跑出来的结果在LB上的得分足足有 \\pm0.005 的差距! 经过各种测试, 定义下面的代码并在开始训练前调用函数即可保证网络结果的可复现性, 要注意除了网络权重随机化造成的影响, 还有是否使用随机种子对训练数据进行shuffle以及处理embedding时缺失单词是否随机生成一个向量等细微之处都会影响最终的结果1234567def seed_torch(seed=1029): random.seed(seed) os.environ['PYTHONHASHSEED'] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True","tags":[]},{"title":"2018腾讯广告算法大赛","date":"2018-08-13T16:00:00.000Z","path":"2018/08/14/腾讯广告算法大赛/","text":"腾讯广告算法大赛 (CTR, Click-Through Rate) 总结 这个比赛是腾讯举办的一个推荐类型的比赛, 训练数据是种子人群及其中用户的脱敏特征, 每个用户的标签是是否属于该种子人群. 因此该问题是一个二分类问题, 采用 AUC 作为评价指标. 初赛阶段的数据量是1000万条数据 (4 GB), 复赛阶段是初赛阶段的5倍, 我们在初赛中的排名是 58/1600 (Top 3.6%), 复赛中由于特征工程较为复杂而数据量增大为5倍, 导致我们所采用的两个模型中的 LightGBM 模型因超出内存导致最终无法完赛. 下面主要介绍本次比赛的一些相关知识点, 分为数据预处理、特征工程和模型选择三个方面. 数据预处理预处理部分主要就是把训练数据处理成计算机模型能够使用的格式. 单、多取值离散特征进行 one-hot 稀疏编码这一步主要就是把离散型单取值和多取值特征进行 one-hot 编码, 比如 age 这一特征是单取值特征, 对所有 age 中出现的数字进行 one-hot 稀疏编码; 而 interest 特征就是多取值特征, 同样也做 one-hot 稀疏编码处理. 部分连续值特征离散化后进行 one-hot 稀疏编码我们注意到部分离散值特征的分布是满足长尾分布或者均匀分布的, 对于长尾分布的连续值特征, 我们将”尾部”统一按0处理, 其他部分则按数量比例划分切割点进行离散化后再 one-hot 编码. 而对于均匀分布的连续值特征则直接均匀的划分并进行 one-hot 编码处理. hashtrick把数据保存为二进制数据流这一部分主要是针对 FFM(Field-aware Factorization Machine) 模型(这个模型会在后文介绍)来设计的, 先把数据以字典的形式处理, 然后经过一个 hash 函数给特征划分 field 及每个 field 内的特征编号. 保存为二进制数据流的形式可以加速读写文件的速度并且占用更小的磁盘空间. 特征工程首先, 在特征工程上我们吃了很大的亏, 这个比赛是有第一届的, 很多强特征挖掘都公布在 Github 上了, 但是我们很遗憾没有想过去挖掘这方面的信息. 另外, 在数据预处理部分中对离散值、连续值的处理也算是特征工程的一部分 特征交叉特征交叉是指把明显具有关联度的特征进行组合以期获取一个更为好的新特征, 我们主要做了双、三特征交叉.比如 age 这一特征和 consumptionAbility(消费能力) 具有明显关联, 一般认为年轻人较中老年人的消费力更强; 而 前两者和 education 学历特征又具有明显的关联, 可以分别做双特征组合, 也可以把这三者组合作为一个新特征. 特征转化率特征转化率是指统计满足某种属性的样本中正样本所占的比例, 例如, 我们可以统计 age=10 的正样本的比例, 但是注意直接这样统计之后作为新特征毫无疑问会暴露标签信息, 学习器会发现所有正样本的比例都符合一定规律. 因此, 我们需要用 k 折交叉来做这个特征. 具体地, 讲述数据分为5折, 前四折数据中统计 age=10 的正样本比例, 对于第5折数据我们使用前四折数据中的统计结果作为特征, 5次轮流对每折数据标记新特征. tfidf特征对于 interest、topic 等多值特征, 我们考虑使用 tfidf 来对这种特征做处理. tfidf(term frequency inverse document frequency)的主要思想是: 如果某个词或短语在一篇文章中出现的频率 tf 高, 并且在其他文章中很少出现, 则认为此词或者短语具有很好的类别区分能力, 适合用来分类. 在这里, 词或短语就是每个 interest 中出现的不同特征值, 文章就是不同样本对应的 interest、topic 多值特征. 模型选择回顾 CTR 领域的模型, 大致发展方向是 LR \\rightarrow GBDT等树模型 \\rightarrow FM/FFM \\rightarrow DeepFM等深度模型.LR 由于太简单, 我们主要尝试了后三个发展阶段的模型, 基于 Baseline 改进的 LightGBM、FFM 和 DeepFM. 最终的情况是 LightGBM 模型效果最好, FFM 模型其次, DeepFM 模型则未能成功使用. 依靠前两个模型的融合得到了初赛58名的成绩. LightGBMLightGBM 模型是 Microsoft 提出的一个 state-of-art 提升树模型, 针对之前的 Xgboost 做出了改善, 通过 GOSS(Gradient-based One-Side Sampling) 和 EFB(Exclusive Feature Bundling)两项技术使得 LightGBM 模型在获取同等精度的情况下速度提升 20倍 以上, 而本次比赛的数据量也确实验证了 LightGBM 的速度优势, 我们尝试了 Xgboost 模型, 同样的训练数据 LightGBM 只用了一天跑完, Xgboost 则根本无法在可接受时间内完成, 此外 LightGBM 的内存占用也优于 Xgboost, 因此我们舍弃了 Xgboost. GOSS: To be continue EFB: To be continue上述是 LightGBM 的理论基础, 因为有 LightGBM 的 Baseline 实现, 针对该模型我们主要做的是参数调优争取在已有的特征基础上获取最好的结果, 查看 LightGBM 的文档, 我们队一些参数进行了调试, 首先影响最大的肯定是学习率参数, 这个参数太大的话速度回变快但是精度降低, 太小的话速度又太慢, num_leaves、max_depth 等参数可以控制过拟合. 为了获取较好的参数, 我们抽取了 10% 的训练数据来做参数优化, 利用这一小部分数据的表现来决定参数最终的配置. FFMFFM 是由 FM 演化而来的, 首先介绍一下 FMFM(Factorization Machine): 降维版本的特征二阶组合FM 之前用的方法是人工特征工程加上 LR, 为了更好地捕捉特征往往需要人为的判断组合一些特征, 例如 “男性大学生” 和 “点击游戏类广告” 两个特征就会是个很好的组合特征. 人工组合特征存在特征爆炸, 特征难以识别等问题. 因此, 提出了线性模型的二阶多项式(2d-polynomial): y_{poly} = \\sigma( + \\sum_{i=1}^{n}\\sum_{j=1}^{n}w_{ij}\\cdot x_i \\cdot x_j)括号内第一部分就是 LR 的模型, 后半部分就是 FM 中扩展的特征组合二次项, 其实就是特征两两相乘(组合)构成新特征, 并对新特征分配一个独立权重, 然后学习这些权重, 将上式改写为矩阵形式: y_{poly} = \\sigma(\\overrightarrow{w}^T \\cdot \\overrightarrow{x} + \\overrightarrow{x}^T \\cdot W^{(2)} \\cdot \\overrightarrow{x})其中W^{(2)} 就是组合特征的权重矩阵, 其参数个数是 O(n^2), 对其进行因子分解(Factorization)得到两个低维(如 n*k)矩阵 W^T \\cdot W 相乘, 至此权重矩阵的参数个数大幅降低为 O(n*k), 这也是 FM 名字的由来. 再对上式中 W^{(2)} 替换成分解后的矩阵并写成向量内积形式: y_{FM} = \\sigma( + )这个式子中后半部分可以看做是 Embedding, 先通过矩阵 W 对稀疏特征 \\overrightarrow{x} 进行嵌入, 随后对嵌入后的稠密向量进行内积得到二阶组合特征. 根据矩阵知识, 上式可进一步写成和式并去除重复的特征平方项, 得到最终的 FM 公式: y_{FM} = \\sigma( + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\cdot x_i \\cdot x_j)其中, \\overrightarrow{v_i} 表示第 i 维特征的隐向量, 其长度为 k. 对比线性模型的二次项推广模型可知, FM 的组合特征权重不是独立的, 因此 FM 是一个参数较少但表达能力较强的模型. FFM(Field-aware Factorization Machine): 基于域的FM通过引入 field 概念, Yu-Chin Juan 等提出了 FM 模型的升级版 FFM, FFM 将相同性质的特征归于同一个 field, 例如 interest 这个多值特征里的所有可能的取值都可以在 one-hot 编码之后归为一个 field. 在 FFM 中, 每一维特征 \\overrightarrow{x_i} 针对其他 field f_j 都会学习一个隐向量 \\overrightarrow{v_{i,f_j}}, 因此隐向量不仅与特征有关, 还与 field 有关, 例如 interest 特征和 age、education 等特征组合时 interest 所使用的隐向量是不同的, 这也符合同一特征与不同的特征之间相关性不同的直觉, 这就是所谓 Field-aware 的由来.那么假设样本有 n 个特征属于 f 个 field, 那么 FFM 有 nf 个隐向量, 而 FM 中只有 n 个, 因此 FM 可以看做是 FFM 的特例即 FM 把所有的特征都归到一个 field 内的 FFM 模型. 我们可以得到 FFM 模型的公式如下: y_{FFM} = \\sigma( + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\cdot x_i \\cdot x_j)其中, f_j 是第 j 个特征所属的 field. 如果隐向量长度为 k, 则 FFM 模型的参数个数为 O(nfk) 远多于 FM 的 O(nk), 其复杂度也因为 field 的引入, 变为 O(kn^2).FFM 模型是流式训练, 对内存占用极低, 不需要人为的挖取特征, 并且训练速度较 LightGBM 也快上许多, 但是精度较 LightGBM 还是差上一些, 初赛近凭 LightGBM 和 FFM 我们就获取了第58名的成绩, 现在看来还是遗憾很大的, 毕竟前50名还有单独的奖励和证书. DeepFM深度模型在这个比赛之前我们几乎没怎么接触过, 所以这个模型最终我们没能成功使用, 但是还是要提一下包括 DeepFM 在内的深度模型是如何解决 CTR 问题的. 首先看下深度模型在 CTR 领域的发展图, 具体参考这里: 一开始 CTR 中的深度模型主要采用 Embedding + MLP 结构, 即对 one-hot 编码后的特征进行 embedding 降维后拼接成一层隐藏层, 随后堆叠全连接层通过一个 sigmoid 得到最后的预测结果. 这种方法对低阶特征挖掘不够学习较为困难.在前面我们提到 FM 模型实际上是通过参数矩阵 W 对特征向量进行 embeddding 然后再通过内积组合特征, 这启发了后人在 DNN 中应用 FM. 下面介绍几个经典的 DNN 模型. Weinan Zhang在2016年提出 FNN(Factorization Machine supported Neural Network), FNN 将 FM 与 MLP 结合, 采用 FM 预训练得到的隐含层及其权重作为神经网络第一层的初始值, 然后堆叠全连接层. 可以看出 FNN 和 Embedding + MLP 结构差距主要在 Embedding 方面, FNN 采用 FM 的预训练权重矩阵作为输入初始值. Huifeng Guo在 2016年提出了 PNN(Product-based Neural Networks), 与 FNN 直接使用 FM 预训练结果不同, PNN 将 FM 预训练得到的嵌入向量两两进行向量积, 得到的结果作为 MLP 的输入. 向量积分为内积和外积操作, 如用内积则将内积后的标量拼接为一个向量作为输入, 如用外积则将得到的矩阵求和作为输入. Google在2016年提出 Wide &amp; Deep 深度模型, 前面的深度模型大多和 FM 有着很强的联系, 而 Wide &amp; Deep 模型则是从 Embedding + MLP 出发, 它奠定了后面 CTR 深度模型(如 DCN、DeepFM)的框架. Wide &amp; Deep 模型将深度模型和线性模型联合训练, 二者结果求和作为预测结果, 其 Deep 部分就是多层 MLP, Wide 部分则是 LR, 可以手动设计特征. Ruoxi Wang在2017年提出了 DCN(Deep &amp; Cross Network), 我们知道 FM 是通过向量内积来实现二阶特征组合, MLP 则是在 Embedding 之后通过一层层的矩阵乘法来组合高阶特征, DCN 的想法就是直接将 FM 过程在高阶特征组合上推广, 其 Deep 部分就是多层 MLP(矩阵操作), Cross 部分则是对 FM 推广(向量操作). Huifeng Guo在2017年提出了 DeepFM, 基于 Wide &amp; Deep 框架, 其 Wide 部分是 FM, 借助 FM 的显式特征交叉帮助 DNN 模型获取更好的表示, 其 Deep 部分的 Embedding 层和 Wide 部分的 FM 共享参数. 总结本次比赛主要还是吃了不懂深度模型的亏, 传统的特征挖掘也不够精细, 另外对 LightGBM 也不算很熟, 复赛阶段由于数据量猛增导致 LightGBM 模型内存超出, 现在想想应该切片训练好歹也是有个结果的.","tags":[]},{"title":"Data Science Bowl","date":"2018-05-24T16:00:00.000Z","path":"2018/05/25/Data Science Bowl比赛/","text":"Kaggle 的 Object Detection 比赛 Data Science Bowl 总结 比赛简介Kaggle的一个目标检测类比赛, 我们也是第一次参加 CV 方向的比赛, 这篇博客主要总结一下和比赛相关的基础知识. 比赛给定一些细胞核的图片, 目标是检测图片中的细胞核的位置, 采用 IOU 作为评价指标, 我们使用的是 Kaiming He 大神的 Mask RCNN, 这个比赛最终我们侥幸拿到了一块银牌(170/3634, Top 5%). 目标检测0、mAP评估指标？mAP (mean Average Precision, 平均精度均值) 是目标检测指标, 在目标检测中, 每一类都可以根据 recall 和 precision 绘制 P-R 曲线, AP 就是该曲线下的面积, mAP 就是所有类 AP 的平均值. 目标检测中, TP 是 IOU&gt;0.5 的检测框数量, FP 是 IOU&lt;0.5 的检测框和对同一个 GT (Ground Truth, 真值框) 多余的检测框数量. FN 是没有被检测出来的GT的数量. 1、IOU是什么？用代码写一个IOU函数IOU的意思是是交并比, 它定义了两个 bbox(bounding box) 的重叠度. 它的公式可以表示为： IOU=\\dfrac{A\\cap B}{A\\cup B}12345678910111213141516171819202122232425#并集def union(au, bu, area_intersection): area_a = (au[2] - au[0]) * (au[3] - au[1]) area_b = (bu[2] - bu[0]) * (bu[3] - bu[1]) area_union = area_a + area_b - area_intersection return area_union#交集def intersection(ai, bi): x = max(ai[0], bi[0]) y = max(ai[1], bi[1]) w = min(ai[2], bi[2]) - x h = min(ai[3], bi[3]) - y if w &lt; 0 or h &lt; 0: return 0 return w*h#交并比def iou(a, b): # a and b should be (x1,y1,x2,y2) if a[0] &gt;= a[2] or a[1] &gt;= a[3] or b[0] &gt;= b[2] or b[1] &gt;= b[3]: return 0.0 area_i = intersection(a, b) area_u = union(a, b, area_i) return float(area_i) / float(area_u + 1e-6) 2、RCNN要介绍 Mask RCNN, 首先就从最开始的 RCNN(Region-based CNN features) 开始, RCNN 将 CNN 的方法引入目标检测领域, 大大提高了检测效果, 改变了目标检测领域的主要研究思路. 其后的 Fast RCNN、Faster RCNN 以及 Mask RCNN 代表了该领域的最高水准.RCNN 的思路主要分为四个步骤:1、候选区域生成: 利用选择性搜索 (Selective Search) 算法从图像中提取 1k~2k 个左右可能是物体的候选区域 (region proposal); 关于选择性搜索算法, 其先将图像分割成小区域 (1k~2k), 然后按照一定的合并规则合并可能性最高的两个相邻区域直到图像只剩一个区域, 最后输出曾经存在过的区域. 2、特征提取: 把这些 region proposal 缩放成 227*227 的大小并用 5 层卷积两层全连接的 CNN 提取特征; 3、类别判断: 用 SVM 训练 CNN 提取出来的各类别特征, 每一类有一个 SVM 分类器; 4、位置精修: 对 SVM 分好类的 region proposal 做边框回归 (bbox regression), 用回归值来矫正建议窗口, 生成预测的窗口坐标. 关于边框回归, 其思想就是平移 + 缩放使得 bbox 更接近 ground truth, 其学习一组参数 W, 使得输入的 bbox X 经过回归变换后和 ground truth Y 非常接近, Y \\approx WX. 缺点: (1) 训练分为多个阶段, 步骤繁琐: 基于预训练网络的微调 + 训练SVM + 训练边框回归器; (2) 训练耗时, 占用磁盘空间大；5000张图像产生几百G的特征文件; (3) 速度慢：使用GPU, VGG16模型处理一张图像需要47s, 如果采用 AlexNet 精度则降低; (4) 测试速度慢：每个候选区域需要运行整个前向CNN计算; (5) SVM和回归是事后操作, 在SVM和回归过程中CNN特征没有被学习更新. 3、SPP NetRCNN 的一大问题就是对于提取的每个 regional proposal 都要经过 CNN 前向传播一次, 计算量巨大. 为此, SPP(Spatial Pyramid Pooling) 则考虑既然 regional proposal 都是从原图像中提取的, 为什么不直接原图像输入 CNN 中, 在映射后的 feature maps 中提取 regional proposal, 通过这样做 SPP 大大减少了计算量.一句话来说就是用 SPP layer 通过三个 Pooling 窗口 (下图中的蓝、绿、灰窗口, 形成了金字塔形状因此得名 SPP) 从 feature maps 提取出同样维度的特征向量. 之后 fast rcnn 的 RoI 层就是借鉴了 SPP Net 的这种思路. 一张图片放入 CNN 网络中经过多层转化后, 会生成多张(论文中是 256 个卷积核)特征图： 而在原图中任意一个窗口在这些特征图中都有对应的窗口, SPP 就是在这些对应的窗口中做池化操作使每张特征图都生成21维的特征向量. 具体做法是, 在 conv5 层得到的特征图是 256 个, 每个都做一次 spatial pyramid pooling. 先把每个特征图分割成多个不同尺寸的网格, 网格分别为 4*4、2*2、1*1 ,然后每个网格做 max pooling, 这样 256 个特征图就形成了 16*256, 4*256, 1*256 维特征, 他们连起来就形成了一个固定长度的特征向量, 将这个向量输入到后面的全连接层. 4、Fast RCNNRCNN、SPP Net 训练时, 提取 region proposal、CNN 特征提取、SVM 分类和 bbox 回归是互相隔离的, Fast RCNN 则有几个特点: 1) 特征都暂存在显存上不需要额外磁盘空间, 速度更快、精度更高; 2) 联合训练, 除了提取 region proposal 阶段, 原来的SVM 分类、bbox 回归都联合在 CNN 阶段同时训练, 将原来最后一层的 softmax 替换成区域分类 softmax (替代 SVM 分类) 和 bbox 微调以完成联合训练目标, 输入变为两个, 一个是原图像作为 CNN 输入, 另一个是选择搜索算法给出的可能 region proposal 直接映射到 Conv5 层上; 3) 提出了 RoI (Region of Interest) 层, 算是 SPP 的变种, 将原来的多尺度 (多个 Pooling 窗口) 映射换成单尺度映射.Fast RCNN 的整体流程如下: 1、利用 selective search 算法在图像中从上到下提取2000个左右的候选窗口 (Region Proposal 也即是 RoI); 2、将整张图片输入 CNN, 进行特征提取; 3、把候选窗口映射到 CNN 的最后一层卷积 feature map 上; 4、通过 RoI pooling 层为每个候选窗口生成固定尺寸 (7*7) 的 RoI 特征向量; 5、利用 Softmax Loss (区域分类损失) 和 Smooth L1 Loss (边框回归损失) 对分类概率和边框回归联合训练. 相比R-CNN, 主要两处不同: (1)最后一层卷积层后加了一个 RoI Pooling layer, RoI 是 SPP Net 的简化版本, SPP Net 对一个区域窗口进行 4*4, 2*2, 1*1 的 pooling, 也就是说一个区域对应的特征图经过 SPP Pooling 最终得到 21 维的特征向量；而 RoI layer 只进行 7*7 的池化, 所以得到的是 49 维的特征向量. (2)损失函数使用了多任务损失函数 (multi-task loss), 将边框回归直接加入到 CNN 网络中训练. 5、Faster RCNNFaster RCNN 可以简单概括成: faster rcnn = fast rcnn + rpn 层, faster rcnn 将 fast rcnn 中 RoI 的提取单独写成了一个 RPN (Region Proposal Network) 层. 其流程如下: 1、将整个图片输入 CNN, 提取图像的 feature maps, 被后续的 RPN 和全连接层所共享; 2、通过 RPN 生成 region proposal, 通过 softmax 判断 anchors 属于 foreground 或 background 并用 bbox 回归修正 anchors 获取精确的 region proposal; 3、RoI Pooling, 根据 feature maps 和 region proposal 提取 proposal feature maps 传给全连接层测过判断类别; 4、根据 proposal feature maps 计算类别并再次使用 bbox 回归获取最终精确的预测位置. RPNFaster RCNN 抛弃选择搜索算法生成候选窗口, 使用 RPN 层选出一些候选区窗口供 fast rcnn 网络的训练和预测, 起加速作用. rpn 的核心是 anchor 机制, 针对经过卷积层之后 feature map 上的每一个点, 再次通过 3*3 的卷积融合包括其在内周围 9 个点的信息形成一个锚点特征向量 (对应下图中的 sliding window 的中心, 作者没有解释这样的做的原因, 推测是结果更具鲁棒性), 融合后维度保持不变仍然是 256 维向量, 随后以锚点为中心设置不同尺寸的 anchor box 以粗略选取原图对应的不同宽高比例, 不同大小的候选区域 (这样选取的结果不够精确, 所以需要两次 bbox 回归修正), 在 faster rcnn 中 anchor box 数量设置为 k=9. 通过引入 anchors 使得 faster rcnn 具备了多尺度映射. RPN 层的处理流程: 原图像经过一个 5 层的卷积网络会得到 H*W*256 的 feature map, 在论文里的 H*W 是原图像的 \\dfrac{1}{16}; 再次通过一个 3*3 的卷积层融合 sliding window 内的 9 个点形成锚点特征向量, 维度为 256 ; 以锚点为中心, 生成 k = 9 个 anchor box 作为初步的候选窗口, 个数 H*W*k; 锚点的特征向量经过两个分支进行训练: 一个分支是候选区域二分类, 经过 cls layer, 得到 2k(k组) 个得分, 每组两个值分别表示 foreground(anchor box 包含检测目标) 和 background(anchor box 不包含检测目标) 的概率; 另一个分支是 bbox regression, 经过 reg layer, 得到 4k(k组) 个输出, 每组的 4 维向量分别表示候选区域对应的四个坐标: (t_x, t_y, t_w, t_h). 其中, 每个 anchor 是否为真实区域取决于 IOU 的大小, 与任意一个真实区域的 IOU &gt; 0.7 则为标定为正样本, 与所有真实区域的 IOU &lt; 0.3 则标定为负样本. 使用所有的 anchors 训练过多, 实际训练中作者采用了 128 个正样本和 128 个负样本来训练. RPN 层和 fast rcnn 是共享特征图的, 只不过 RPN 层是利用 anchor 机制事先选择得到候选框. 这部分候选框映射到的特征图的区域经过 ROI pooling 得到相同大小的特征向量, 传递给后面两个分支: 全连接层分类和 bbox 回归. 6、Mask RCNN终于到了正题 Mask RCNN, 这是我们在比赛中所使用的 state-of-art 模型, Kaiming He 大神在 2018 年初提出的模型.mask rcnn 在 faster rcnn 工作的基础上提出的同时可以进行目标检测和目标分割的模型, 可以认为 Mask RCNN = ResNet-FPN + Fast RCNN + RoIAlign + Mask分支. 而 ResNet-FPN + Fast RCNN 实际上就是 Faster RCNN, 只不过 Faster RCNN 中只有一个 featrue map(conv5 对应的 featrue map), 而 ResNet-FPN 对应的是多个 feature map (conv2、conv3、conv4、conv5 构成的金字塔). FPNFPN (Feature Pyramid Networks, 特征金字塔网络) 的提出是为了解决图像的多尺度目标检测问题. 在目标检测中, 一个目标类别可能有很多种尺度, 比如在街头, 远处的行人和近处的行人尺度就相差很大. 这种多尺度问题用传统网络来做精度不够令人满意. 因为目标图像再经过深层卷积网络后, 语义信息增强但位置信息减弱, 所以最终检测效果会受到影响. 一个简单的思路是把目标图像进行多尺度缩放, 一起交给模型进行训练. 显然这种方法十分耗费计算资源和训练时间. 所以作者希望提出一种网络结构, 让图像在经过网络后既能提取出深层的语义信息, 也能保留原图的位置信息. 图像在经过一些卷积层和池化层后, 一般都是通道数变多, 特征图变小. 参见 resnet, Mask RCNN 中使用的 FPN 就是 ResNet-FPN: 可以看出, 卷积层 [conv2, conv3, conv4, conv5] 特征图每次长宽缩小为原来的 \\frac{1}{2}. 这个过程可以看成金字塔结构由底向上走, 图像的坐标信息逐渐减弱, 但语义信息随着通道数的增加而增强. 这就是 FPN 的第一阶段, 金字塔向上走的过程. 接下来是 FPN 的第二阶段：自顶向下——深层的特征图和浅层特征图通过横向连接融合. 如下图所示, 深(顶)层的特征图通过最近邻上采样变换成原来的2倍, 下一层的特征图进行 1*1 卷积降低通道数保持和上层一致, 通过横向连接两层融合, 逐层往下走. 这个过程使图像的深层语义信息得以保留, 并且引入了底层的位置信息. 到了底部后, 再进行一次 3*3 的卷积, 去除上采样的混叠现象. RoIAlignmask rcnn 不仅需要检测候选框的类别, 还需要给出目标类别的 mask 映射. mask 映射应该遵循平移等价性, 就是说框移动, 输出的 mask 也会相应移动 (平移不变性是指框移动, 输出也不会变化, 比如分类器). 所以, 传统的 faster rcnn 在这一点上有着较大的缺陷, 因为它在卷积过程和 ROI 层经过了两次量化损失. 比如说, 原图像 256*256, 而特征图是 16*16, 缩小为原来的 \\frac{1}{16}. 那么原图中坐标 17 映射到特征图上就是 1.0625, 小数部分会被舍去. 而在 ROI 层, 如果框的大小是 15*15, 要进行 7*7 的 pooling, 又是一次损失. 这种损失会导致生成的 mask 和目标无法对齐. 所以, RoIAlign 为了得到浮点数坐标的像素值, 利用了双线性插值算法. 像上图种每个 bin 里面的四个点的像素值, 都是由这种方法得到的, 之后再进行正常的 max pooling 操作. mask rcnn structuremask rcnn 和 faster rcnn 除了 RoIAlign 的区别外, 另一个区别就是加了一个 mask 的分支, 如下图： 以 ResNet 为例：候选区域经过 RoIAlign 层得到 7*7*2048 的 feature map, mask 分支是通过全连接层网络来实现的, 最终得到了 14*14*80 的输出, 这里的 80 就是类别数, 也就是有 80 个 mask, 但只有一个是对应于目标类别的. 所以需要等另一个分支预测得到区域类别后选择对应的 mask 输出. 7、yoloone-stage目标检测的主流方法, 速度比rcnn系列模型要快很多, 在一些注重速度的目标检测系统中经常会用到. yolo v1该框架假设一张448*448的图片被划分成S*S的网格 (论文中S=7), 经过如下的网络结构后输出了S*S*N的向量, 每个N维向量对应于一个格子的预测信息. 其中N=B*5+C, B是认为设置的要预测的bbox数量, C是目标类别数 (论文中分别是2和20)；所以N=30. 这里记录几个关键点： 其中bbox的位置信息是一个5维向量：(x, y, w, h), 类别预测向量是20维. 坐标x,y代表了预测的bbox的中心与栅格边界的相对值. 坐标w,h代表了预测的bounding box的width、height相对于整幅图像width,height的比例. ground truth box只有一个, 要同时预测多个bbox信息的目的是起到一种集成的作用. 这些bbox位置大小信息的输出在训练过程中都是以同一个ground truth box的信息来计算loss的. 输出类别概率的公式为： \\operatorname { Pr } \\left( \\text { Class } _ { i } | \\text { Object } \\right) * \\operatorname { Pr } ( \\text { Object } ) * \\text {IOU}_{\\text{pred}}^{\\text{truth}} = \\operatorname { Pr } \\left( \\text { Class } _ { i } \\right) * \\text{IOU}_{\\text{pred}}^{\\text{truth}} 因为每个格子都会给出两个bbox的预测, 所以在一张图片中每个目标类别会得到7*7*2个bbox和概率值. 要得到最终的bbox还得经过一个后处理步骤: NMS(非极大值抑制, 去除冗余 bbox), NMS 首先丢弃分类概率小于预定值的 bbox, 然后对不同 bbox 计算的分类概率排序, 选择最大概率的 bbox 假设是 A 加入最终检测结果, 判断剩余的 bbox, 只要其与 A 的 IOU 大于预设阈值就丢弃, 重复上述过程直到候选框数量为0. yolo v28、SSD除了yolo, SSD是目标检测领域one-stage方法的另一主流框架, 和yolo的区别如下： 1、yolo只用最高层的特征图 (7*7*1024)来进行预测, 这样做速度很快但在检测小物体上就不尽人意. SSD效仿了特征金字塔的思路, 在6种尺度的特征图上进行softmax分类和bbox回归； 2、SSD还加入了Prior box, 类似于RPN的anchor； structure conv4_3网络分为了3条线路: 1、经过bn层+一次卷积后进行softmax分类, 预测目标类别；因为特征图大小为38*38*512, 每个位置对应于一个512维的特征向量, 所以相当于进行38*38次512维向量的softmax分类. 2、经过bn层后+一次卷积后特征图每个点生成了4*num_priorbox大小的特征, 这些特征用于后面bbox regression. 3、进行bbox regression, 特征图每个点经过regressor生成2*4*num_priorbox维度的输出. 每个prior box得到一组(x1,y1,x2,y2)和四个坐标值的方差预测, 8个数. Prior box SSD按照如下规则生成prior box： 以feature map上每个点为中心, 生成一系列同心的prior box 正方形prior box最小边长和最大边长为： min\\_size\\\\ \\sqrt{min\\_size*max\\_size}\\sqrt{aspect\\_ratio}*min\\_size\\\\ 1/\\sqrt{aspect\\_ratio}*min\\_size每个feature map对应的min_size和max_size由以下公式决定： 对于conv4_3：k = 1, min_size = s1*300, max_size = s2*300对于conv-7：k = 2, min_size = s2*300, max_size = s3*300 9、Focal LossFocal loss是针对交叉熵在样本不平衡的场景下做出的改进, 简单来说就是给负样本加了一个能随分类难度变化而变化的权重, 从而让易分类的样本对loss的贡献变小, 让难分类样本和数量少的类别对loss的贡献变大. CE(p,y)=CE(p_t)=-log\\ p_t\\\\ \\text{subject to: } p_t=\\left\\{\\begin{array}{cc} p,& if\\ \\ y=1\\\\ 1-p,& otherwise \\end{array}\\right.上面的公式是交叉熵损失 (对于类别t的loss), 是目标检测中常用的损失函数. 对于two-stage方法而言, 这个损失函数并没有什么不足因为有RPN进行初步的候选区域采样. 所以对于后面的网络, 正负样本比例不会有严重失衡的情况. 但对于one-stage方法来说, 采样得到的样本框大部分都是背景, 这种情况下负样本占了loss的大部分贡献, 会影响正样本的分类性能. 一个直观的方法是在样本少的类别前面加上一个大于1的权重: CE(p_t)=-\\alpha_tlog\\ p_t但这种办法只能解决样本少的类别问题, 并没有解决样本分类难易的问题. 因此有了如下的形式: FL(p_t)=-(1-p_t)^{\\gamma}log\\ p_t对于预测概率p_t高的样本, FL loss认为它是容易分类的样本, 因此给的权重就比较低. 而对于预测概率比较低的样本, FL loss认为它们难以分类, 给的权重也比较高；所以模型会侧重训练这些样本. 综合以上两种形式可得: FL(p_t)=-\\alpha_t(1-p_t)^{\\gamma}log\\ p_t","tags":[]},{"title":"机器学习算法面经总结","date":"2018-03-15T11:46:34.048Z","path":"2018/03/15/\b\b知识总结/","text":"记录机器学习/深度学习/算法岗的面试经验, 包含基础知识和算法题 算法岗基础知识反向传播原理简介: 反向传播实际上就是对复合函数的链式求导, 神经网络本质上可以理解为函数嵌套, 根据输入这个函数有自己的输出可能会和真实输出不等, 不等就会产生误差, 反向传播就是使用梯度下降作为优化方法更新这个函数的参数使得产生的误差最小化. 反向传播通过由后向前的的方式避免了对求导路径的重复访问因此可以十分高效的计算偏导. 即所谓的正向传播预测反向传播误差. Sigmoid和ReLU激活函数的优劣: 使用激活函数的目的是引入非线性变换, 普遍认为ReLU比sigmoid更好, 因为sigmoid函数有三个主要缺点: 一是sigmoid函数在输入很大或很小的时候梯度接近与0, 产生梯度消失现象;二是sigmoid函数的在求导时涉及到除法运算, 计算量很大; 三是sigmoid函数的输出不是0均值的, 即如果输入x总是为正的话, 对于式 f = wx+b, 关于w的梯度总是非负即正(损失函数对w的导数总是和损失函数对f的导数的符号一致). ReLU函数则计算速度快且由于梯度始终不饱和收敛速度更快没有梯度消失现象, 但是ReLU函数也是非0均值的输出, 且易出现神经元”Dead”现象即由于负梯度到该神经元被置0参数不再被更新并且该神经元可能永远无法再被激活. 梯度消失问题和损失函数有关吗? 基本上无关, 梯度消失问题主要和网络层数和激活函数的导数有关, 层数太大又不实用batch normalization的以及如果选用sigmoid激活函数则很有可能造成梯度消失现象. 使用TensorFlow训练神经网络模型的基本流程: 训练神经网络可分为以下三个步骤 1 定义神经网络的结构和前向传播的输出结果 2 定义损失函数以及选择后向传播的算法 3 生成会话 并且在训练数据上反复执行反向传播优化算法.无论神经网络结构如何变化, 这三个步骤基本不变. XGboost和GBDT的区别:Xgboost防止过拟合的方法: 一是目标函数中加入了正则项, 包括对叶节点数目的惩罚以及叶节点值的惩罚(取值不能太极端); 二是采用了RF中的列采样策略; 三是Shrinkage, 对应的就是eta参数, 一次迭代后将叶子节点的权值乘上这个系数, 目的是为了削弱单颗树对结果的影响, 让后面的树可以有更大的发挥空间.reference here XGboost和RF的区别: reference here ROC、AUC等概念的理解: reference here一句话说明AUC的本质和计算规则：给定一个正例, 一个负例, 预测为正的概率值比预测为负的概率值还要大的可能性. K-means、KNN、层次聚类和密度聚类(DBSCAN):这个链接直接对比了Kmeans和KNNK-means &amp; KNN层次聚类的复杂度很高, 但是一次聚类完成就可以得到一个聚类树结果, 选择不同的聚类个数下的聚类结果直接根据树来得到即可且层次聚类是贪心的思想得到的是局部最优需要随机化, kmans的主要优势是效率K-means &amp; 层次聚类DBSCAN是基于密度的聚类, 对于密度稀疏的数据表现较差, 但是可以运用到凸和非凸数据集上(甜甜圈数据集), 其计算复杂度较高但是结果往往要比kmeans好, DBSCAN对异常点也不敏感, 聚类结果可复现, 但同时需要调参达到最佳性能DBSCAN原理K-means &amp; DBSCAN 梯度优化的几种方法:按照大类来分包括: 一阶优化和二阶优化, 二阶优化由于用到了二阶导数计算代价较大往往不会广泛使用(代表算法有牛顿法), 只看一阶优化算法主要就是梯度下降包括SGD、BGD、mini-batch GD(MBGD有时也称为SGD)、Momentum、AdaGrad、Adam等等.refrence hererefrence here 数据偏斜的处理:在有数据有偏的情况下, 评价标准也需要更换比如ROC, 如果需要特别关注样本较少的类别, 那么precision-recall曲线则是一个很好的选择; 将数据聚类对大类别数据进行欠采样, 对小类别数据进行相似生成或过采样, 调整分类器的阈值: 少类别样本/(少类别样本+多类别样本), 欠采样或是过采样都会导致数据分布被改变因此使用集成学习的方法来调整分类阈值是一个很好的选择 GBDT+LR的原理:利用GBDT来做embedding特征, 预先选定GBDT的树和叶子的数量就可以设置好用于LR分类的特征维度(N_{trees} * N_{leaves}), \b通过one-hot编码即可作为LR的输入特征来使用 GBDT+LR中, 如果GBDT有有1万颗树, 每个树有100个叶子节点, 那么输入到LR的特征会是一个高维稀疏的向量, 那么应该如何处理, 使用PCA降维的话会造成损失, 如果不想有损失的话应该怎么办:??????????????????????????????? GBDT在回归和多分类当中有什么不同, 在预测的时候的流程是怎样的:在回归和多分类任务中的主要区别在于损失函数不同造成的算法初始化不同以及\b叶节点取值和表示含义的不同, 回归任务常用均方损失函数、绝对值损失函数和Huber损失函数(前面两者的折中), 而多分类则采用多类逻辑损失函数(二分类是对数损失函数), 对于回归\b类问题, 定义好损失函数之后, GBDT算法\b的预测值表示与真实值的偏差, 而多分类中的预测值则表示预测值为真实值的概率. 此外对于多分类, GBDT采用一对多策略, 训练过程中的主要区别是多分类中多了一层内部for循环(k个类别都各自拟合完一棵树之后再拟合下一棵树, 一共拟合M轮, 最终有M*K颗树), 最后使用softmax来计算最终的类别概率详细的介绍 逻辑回归如何防止过拟合:减少特征的数量; 正则化, 加入模型复杂度的惩罚项 L1、L2正则化的区别是什么:L1正则化是参数的绝对值之和(Lasso回归), L2正则化是参数的平方和后再开根值(岭回归). 它们都可以防止过拟合, 区别在于L1正则化倾向于稀疏(部分特征的权值会为0)而L2正则化则是使每个权值都很小接近于0但不会为0, L1的稀疏特性使得L1正则可以用于特征选择. L1范数符合拉普拉斯分布在一些地方是不可导的, 用有约束的优化来解释就是: 在图像中的特点就是有很多棱角(棱角处包含很多0, 高维情况下尤为明显), 这些棱角有更大概率和误差项等值线相交从而出现很多0, 而L2范数符合高斯分布完全可导不会那么多棱角. L1正则不可导该如何解决:Proximal Algorithms或者ADMM来解决 坐标轴下降法的原理是什么:坐标下降法属于一种非梯度优化的方法, 它在每步迭代中沿一个坐标的方向进行线性搜索(线性搜索是不需要求导数的), 通过循环使用不同的坐标来达到目标函数的局部极小值, 简单来说就是固定一个变量把其他变量当做常量看待 LSTM和GRU的区别:LSTMLSTM&amp;GRU双向LSTM GRU和LSTM的性能在很多任务上不分伯仲. GRU参数更少因此更容易收敛, 但是数据集很大的情况下, LSTM表达性能更好. 从结构上来说, GRU只有两个门(update和reset), LSTM有三个门(forget, input, output), GRU直接将hidden state传给下一个单元, 而LSTM则用memory cell把hidden state包装起来. LSTM防止梯度弥散和爆炸的原因:传统的RNN是用覆盖的的方式计算状态: S_t = f(S_{t-1}, x_t), 这有点类似于复合函数, 那么根据链式求导的法则, 复合函数求导：设 f 和 g 为 x 的可导函数, 则 (f \\circ g)'(x) = f'(g(x))g'(x), 他们是一种乘积的方式, 那么如果导数都是小数或者都是大于1的数的话, 就会使得总的梯度发生vanishing或explosion的情况, 当然梯度爆炸(gradient explosion)不是个严重的问题, 一般靠裁剪后的优化算法即可解决, 比如gradient clipping(如果梯度的范数大于某个给定值, 将梯度同比收缩), 但是梯度消失做不到, 这个时候就要用lstm了. 在lstm中, 我们发现, 状态 S 是通过累加的方式来计算的, S_t = \\sum\\limits_{i=1}^t \\Delta S_i. 那这样的话, 就不是一直复合函数的形式了, 它的的导数也不是乘积的形式, 这样就不会发生梯度消失的情况了. boosting和bagging在不同情况下的选用:boosting和bagging的区别在于boosting能够改善bias而bagging能改善variance, 所以对于高bias低variance的模型选用boosting, 对于低bias高variance的模型采用bagging, 如果有时间限制考虑到boosting只能串行训练可以使用bagging并行的对各个模型训练. 根据自己的经验, 只要模型不容易过拟合boosting的表现一般都不差于bagging Seq2seq模型:经典的seq2seq模型就是翻译模型, 例如google翻译团队的transformer + attention. seq2seq模型最大的特点是突破了传统RNN模型输入输出等长的限制. 而seq2seq模型实际上属于encoder-decoder结构, encoder将输入序列压缩成指定维度的隐向量, decoder根据这个隐向量生成输出序列, 生成输出序列可以仅把隐向量当做decoder的输入, 也可以把隐向量加入到生成序列的每个时刻的计算中. 算法题 数组中第k大的数 通过有偏概率0/1生成器, 生成无偏概率0/1生成器 旋转数组求最小值 益智题 站在地球上的某一点, 向南走一公里, 然后向东走一公里, 最后向北走一公里, 回到了原点.地球上有多少个满足这样条件的点?(无数个)","tags":[]},{"title":"Spooky Author Classification","date":"2018-01-02T16:00:00.000Z","path":"2018/01/03/Spooky比赛/","text":"Kaggle 的 NLP 比赛 Spooky Author Classification 总结","tags":[]}]